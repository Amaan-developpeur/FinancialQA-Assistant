{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74893304-385d-4328-8c91-410bbf503273",
   "metadata": {},
   "source": [
    "# Step 1: Text Extraction from the PDF's #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fba491f-cc35-4076-9e29-90705b86c3ee",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>\n",
    "        In this step there are 2 functions which make the logic of <b>extract_text_from_pdf.py.</br>\n",
    "    </li>\n",
    "</ul>\n",
    "<ol>\n",
    "    <li>extract_chunks_from_page: This function extracts chunks from a single page</li>\n",
    "    <li>extract_text_from_pdf: This function is responsible for extracting text, calling the extract_chunks_from_page function to extract chunks in a loop</li>\n",
    "</ol>\n",
    "    </li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f748b8eb-722c-4d6a-bccb-c089cec7f5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting PDFs: 100%|███████████████████████████████████████████████████████████████████| 8/8 [08:45<00:00, 65.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Extracted 3311 chunks to C:\\Users\\DELL\\Data Science\\Deep Learning\\NLP\\Projects\\financial-qa\\data\\extracted_chunks.csv\n",
      "No extraction errors encountered.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "#--------------Directory paths---------------------------------\n",
    "BASE_DIR = Path().resolve().parent\n",
    "RAW_DIR = BASE_DIR / \"data\" / \"raw_docs\"\n",
    "OUTPUT_FILE = BASE_DIR / \"data\" / \"extracted_chunks.csv\"\n",
    "ERROR_LOG_FILE = BASE_DIR / \"data\" / \"error_log.csv\"\n",
    "\n",
    "\n",
    "# Extracts text from a single page.\n",
    "# Since this will get called in the below function, chunks from all the pages gets generated.\n",
    "\n",
    "def extract_chunks_from_page(text, filename, page_num, chunk_size=300, overlap=50):\n",
    "    words = text.strip().split()\n",
    "    chunks = [] # In this variable all the extracted chunks will get stored of the specific page\n",
    "    for x in range(0, len(words), chunk_size - overlap):\n",
    "        chunk_words = words[x:x + chunk_size]\n",
    "        if len(chunk_words) < 30:\n",
    "            continue  # skip very short fragments\n",
    "        chunk_text = \" \".join(chunk_words)\n",
    "        chunk_id = f\"{filename}_p{page_num}_c{x}\"\n",
    "        chunks.append({\n",
    "            \"filename\": filename,\n",
    "            \"page\": page_num,\n",
    "            \"chunk_id\": chunk_id,\n",
    "            \"text\": chunk_text\n",
    "        })\n",
    "    return chunks\n",
    "\n",
    "def extract_text_from_pdf(raw_dir):\n",
    "    all_chunks = [] # Chunks from each page extracted using the function \"extract_chunks_from_page\" gets stored here.\n",
    "    error_log = []\n",
    "    pdf_files = [f for f in os.listdir(raw_dir) if f.lower().endswith(\".pdf\")]\n",
    "\n",
    "    for filename in tqdm(pdf_files, desc=\"Extracting PDFs\"):\n",
    "        file_path = os.path.join(raw_dir, filename)\n",
    "\n",
    "        try:\n",
    "            with pdfplumber.open(file_path) as pdf:\n",
    "                for page_num, page in enumerate(pdf.pages, start=1):\n",
    "                    try:\n",
    "                        text = page.extract_text()\n",
    "                        if not text:\n",
    "                            continue\n",
    "                        text = text.replace(\"\\n\", \" \").strip()\n",
    "                        chunks = extract_chunks_from_page(text, filename, page_num)\n",
    "                        all_chunks.extend(chunks)\n",
    "                    except Exception as page_err:\n",
    "                        error_log.append({\n",
    "                            \"filename\": filename,\n",
    "                            \"page\": page_num,\n",
    "                            \"error\": str(page_err)\n",
    "                        })\n",
    "        except Exception as file_err:\n",
    "            error_log.append({\n",
    "                \"filename\": filename,\n",
    "                \"page\": -1,\n",
    "                \"error\": f\"Failed to open PDF: {file_err}\"\n",
    "            })\n",
    "\n",
    "    return all_chunks, error_log\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chunks, errors = extract_text_from_pdf(RAW_DIR)\n",
    "    \n",
    "    # Saving the extracted chunks\n",
    "    df = pd.DataFrame(chunks)\n",
    "    df.to_csv(OUTPUT_FILE, index=False)\n",
    "    print(f\"Done. Extracted {len(df)} chunks to {OUTPUT_FILE}\")\n",
    "    \n",
    "    # Saving any errors if they occur\n",
    "    if errors:\n",
    "        error_df = pd.DataFrame(errors)\n",
    "        error_df.to_csv(ERROR_LOG_FILE, index=False)\n",
    "        print(f\"Logged {len(error_df)} errors to {ERROR_LOG_FILE}\")\n",
    "    else:\n",
    "        print(\"No extraction errors encountered.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dfce5d-a6c3-42dc-9025-dd581009bd4d",
   "metadata": {},
   "source": [
    "### Inspection of Chunks ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab698dbb-60fa-4cc3-a6fe-6c51ab55fac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3311.000000\n",
       "mean      232.308064\n",
       "std        86.817985\n",
       "min        30.000000\n",
       "25%       166.000000\n",
       "50%       300.000000\n",
       "75%       300.000000\n",
       "max       300.000000\n",
       "Name: word_count, dtype: float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#--- Chunk length and distribution ----\n",
    "\n",
    "df['word_count'] = df['text'].str.split().apply(len)\n",
    "df['word_count'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a34b09d-b33f-4410-ac55-950ab321fb37",
   "metadata": {},
   "source": [
    "<b> Models usually can process around ~512 tokens. Here in the chunks that I've created doesnot exceed that limit as max=300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39312832-12ee-483f-83fe-108648cdfd8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>page</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>text</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [filename, page, chunk_id, text, word_count]\n",
       "Index: []"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#--- Empty and Garbage chunk -------\n",
    "\n",
    "df[df['text'].str.strip() == '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "007b088d-bc78-4ff6-be3e-15615d027a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>page</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>text</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [filename, page, chunk_id, text, word_count]\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#--- Chunks with only Numbers, Non-word Characters and Punchuations -------\n",
    "\n",
    "import re\n",
    "\n",
    "pattern = r'^[\\W\\d\\s]+$'  # non-word characters, digits, and whitespace\n",
    "garbage_chunks = df[df['text'].str.match(pattern)]\n",
    "garbage_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fab3bd3-1f06-474a-bcaa-ffbd33c8db8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>page</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>unique_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [filename, page, chunk_id, text, word_count, unique_words]\n",
       "Index: []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#----- Chunks with very few unique words --------\n",
    "\n",
    "df['unique_words'] = df['text'].apply(lambda x: len(set(x.lower().split())))\n",
    "df[df['unique_words'] < 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5c861d9-328d-4d90-bca4-89b1edce4a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dc1d6d9-a402-4710-a5a4-92addce08202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>page</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>unique_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HDFC_AGM_Transcript_Aug2024.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>HDFC_AGM_Transcript_Aug2024.pdf_p1_c0</td>\n",
       "      <td>CIN: L65920MH1994PLC080618 HDFC Bank Limited, ...</td>\n",
       "      <td>300</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HDFC_AGM_Transcript_Aug2024.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>HDFC_AGM_Transcript_Aug2024.pdf_p1_c250</td>\n",
       "      <td>by way of remote e-voting will not be able to ...</td>\n",
       "      <td>143</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HDFC_AGM_Transcript_Aug2024.pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>HDFC_AGM_Transcript_Aug2024.pdf_p2_c0</td>\n",
       "      <td>resolutions as read. Now, without any further ...</td>\n",
       "      <td>280</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HDFC_AGM_Transcript_Aug2024.pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>HDFC_AGM_Transcript_Aug2024.pdf_p2_c250</td>\n",
       "      <td>present. With permission of the members, I cal...</td>\n",
       "      <td>30</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HDFC_AGM_Transcript_Aug2024.pdf</td>\n",
       "      <td>3</td>\n",
       "      <td>HDFC_AGM_Transcript_Aug2024.pdf_p3_c0</td>\n",
       "      <td>Friends, we would like to recall last year whe...</td>\n",
       "      <td>300</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          filename  page  \\\n",
       "0  HDFC_AGM_Transcript_Aug2024.pdf     1   \n",
       "1  HDFC_AGM_Transcript_Aug2024.pdf     1   \n",
       "2  HDFC_AGM_Transcript_Aug2024.pdf     2   \n",
       "3  HDFC_AGM_Transcript_Aug2024.pdf     2   \n",
       "4  HDFC_AGM_Transcript_Aug2024.pdf     3   \n",
       "\n",
       "                                  chunk_id  \\\n",
       "0    HDFC_AGM_Transcript_Aug2024.pdf_p1_c0   \n",
       "1  HDFC_AGM_Transcript_Aug2024.pdf_p1_c250   \n",
       "2    HDFC_AGM_Transcript_Aug2024.pdf_p2_c0   \n",
       "3  HDFC_AGM_Transcript_Aug2024.pdf_p2_c250   \n",
       "4    HDFC_AGM_Transcript_Aug2024.pdf_p3_c0   \n",
       "\n",
       "                                                text  word_count  unique_words  \n",
       "0  CIN: L65920MH1994PLC080618 HDFC Bank Limited, ...         300           161  \n",
       "1  by way of remote e-voting will not be able to ...         143           100  \n",
       "2  resolutions as read. Now, without any further ...         280           156  \n",
       "3  present. With permission of the members, I cal...          30            28  \n",
       "4  Friends, we would like to recall last year whe...         300           178  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1950702d-5dc2-4223-9e29-a657580ecbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"word_count\", \"unique_words\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5e38c7-8722-4c77-9d07-8d50c824e243",
   "metadata": {},
   "source": [
    "# Step 2: Generating the Embeddings from the Extracted Chunks #\n",
    "\n",
    "<ul>\n",
    "    <li>\n",
    "        The below cell contains the logic of generate_embeddings.py.\n",
    "    </li>\n",
    "    <li>\n",
    "        Embedding simply means converting pieces of text in Vector of Numbers.\n",
    "    </li>\n",
    "    <li>\n",
    "        We'll be using SentenceTransformer to capture the Semantic Context of the sentences properly and is much better than Word2Vec\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e805513d-e079-4de5-a196-9bbb967f84e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e383f648816348bfadd2e026f5e33148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 3311 embeddings to C:\\Users\\DELL\\Data Science\\Deep Learning\\NLP\\Projects\\financial-qa\\data\\embedded_chunks.npy\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "#--------Directory Paths-----------------------\n",
    "BASE_DIR = Path().resolve().parent\n",
    "CHUNKS_CSV = BASE_DIR / \"data\" / \"extracted_chunks.csv\"\n",
    "EMBEDDINGS_OUTPUT = BASE_DIR / \"data\" / \"embedded_chunks.npy\"\n",
    "METADATA_OUTPUT = BASE_DIR / \"data\" / \"embedded_chunks.csv\"\n",
    "\n",
    "# Loading the  data\n",
    "df_embed = pd.read_csv(CHUNKS_CSV)\n",
    "texts = df_embed['text'].tolist()\n",
    "\n",
    "# Loading model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Encoding the data, this will create a vector of numbers of each sentence\n",
    "embeddings = model.encode(texts, batch_size=32, show_progress_bar=True)\n",
    "\n",
    "# Doing vstack is important, because it is important when we're planning to integrate the data with LLM's in the future\n",
    "# It helps in calculating the cosine similarity and LLM's expect input in a certain way.\n",
    "embedding_matrix = np.vstack(embeddings)\n",
    "np.save(EMBEDDINGS_OUTPUT, embedding_matrix)\n",
    "\n",
    "# Save metadata with index\n",
    "df_embed['embedding_index'] = range(len(df_embed))\n",
    "df_embed.to_csv(METADATA_OUTPUT, index=False)\n",
    "\n",
    "print(f\"Saved {embedding_matrix.shape[0]} embeddings to {EMBEDDINGS_OUTPUT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53058831-8f60-44dd-a4f7-3440c05f2a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>page</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>text</th>\n",
       "      <th>embedding_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HDFC_AGM_Transcript_Aug2024.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>HDFC_AGM_Transcript_Aug2024.pdf_p1_c0</td>\n",
       "      <td>CIN: L65920MH1994PLC080618 HDFC Bank Limited, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HDFC_AGM_Transcript_Aug2024.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>HDFC_AGM_Transcript_Aug2024.pdf_p1_c250</td>\n",
       "      <td>by way of remote e-voting will not be able to ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HDFC_AGM_Transcript_Aug2024.pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>HDFC_AGM_Transcript_Aug2024.pdf_p2_c0</td>\n",
       "      <td>resolutions as read. Now, without any further ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HDFC_AGM_Transcript_Aug2024.pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>HDFC_AGM_Transcript_Aug2024.pdf_p2_c250</td>\n",
       "      <td>present. With permission of the members, I cal...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HDFC_AGM_Transcript_Aug2024.pdf</td>\n",
       "      <td>3</td>\n",
       "      <td>HDFC_AGM_Transcript_Aug2024.pdf_p3_c0</td>\n",
       "      <td>Friends, we would like to recall last year whe...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          filename  page  \\\n",
       "0  HDFC_AGM_Transcript_Aug2024.pdf     1   \n",
       "1  HDFC_AGM_Transcript_Aug2024.pdf     1   \n",
       "2  HDFC_AGM_Transcript_Aug2024.pdf     2   \n",
       "3  HDFC_AGM_Transcript_Aug2024.pdf     2   \n",
       "4  HDFC_AGM_Transcript_Aug2024.pdf     3   \n",
       "\n",
       "                                  chunk_id  \\\n",
       "0    HDFC_AGM_Transcript_Aug2024.pdf_p1_c0   \n",
       "1  HDFC_AGM_Transcript_Aug2024.pdf_p1_c250   \n",
       "2    HDFC_AGM_Transcript_Aug2024.pdf_p2_c0   \n",
       "3  HDFC_AGM_Transcript_Aug2024.pdf_p2_c250   \n",
       "4    HDFC_AGM_Transcript_Aug2024.pdf_p3_c0   \n",
       "\n",
       "                                                text  embedding_index  \n",
       "0  CIN: L65920MH1994PLC080618 HDFC Bank Limited, ...                0  \n",
       "1  by way of remote e-voting will not be able to ...                1  \n",
       "2  resolutions as read. Now, without any further ...                2  \n",
       "3  present. With permission of the members, I cal...                3  \n",
       "4  Friends, we would like to recall last year whe...                4  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_embed = pd.read_csv(r\"C:\\Users\\DELL\\Data Science\\Deep Learning\\NLP\\Projects\\financial-qa\\data\\embedded_chunks.csv\")\n",
    "df_embed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895c1c87-5e25-43d4-98bc-aced416084d3",
   "metadata": {},
   "source": [
    "# Step 3: Vector Search #\n",
    "<ul>\n",
    "    <li>\n",
    "        The function below takes a user query, creates an embedding for it, and compares it with the precomputed embedding matrix (from document chunks).\n",
    "It returns the top-k most relevant chunks from extracted_chunks.csv based on cosine similarity.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11c85464-f064-4af8-9704-b7c3863908be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     filename  page  \\\n",
      "0      HDFC_Integrated_Annual_Report_2022.pdf    17   \n",
      "1  HDFC_Integrated_Annual_Report_2024.pdf.pdf   287   \n",
      "2      HDFC_Integrated_Annual_Report_2022.pdf   218   \n",
      "3  HDFC_Integrated_Annual_Report_2024.pdf.pdf   216   \n",
      "4      HDFC_Integrated_Annual_Report_2022.pdf   129   \n",
      "\n",
      "                                            chunk_id  \\\n",
      "0    HDFC_Integrated_Annual_Report_2022.pdf_p17_c250   \n",
      "1  HDFC_Integrated_Annual_Report_2024.pdf.pdf_p28...   \n",
      "2     HDFC_Integrated_Annual_Report_2022.pdf_p218_c0   \n",
      "3  HDFC_Integrated_Annual_Report_2024.pdf.pdf_p21...   \n",
      "4   HDFC_Integrated_Annual_Report_2022.pdf_p129_c250   \n",
      "\n",
      "                                                text  similarity  \n",
      "0  been increased to 72.7% and the coverage of to...    0.588073  \n",
      "1  STANDALONE PROFIT AND LOSS ACCOUNT For the yea...    0.556045  \n",
      "2  Profit and Loss Account For the year ended Mar...    0.535019  \n",
      "3  finally paid off for some critical sectors may...    0.512183  \n",
      "4  diversified loan book across a contraction of ...    0.508816  \n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#------------Loading the Model-------------------------\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")  \n",
    "\n",
    "BASE_DIR = Path().resolve().parent\n",
    "embeddings = np.load(BASE_DIR/ \"data\" / \"embedded_chunks.npy\")\n",
    "df_chunks = pd.read_csv(BASE_DIR / \"data\" / \"extracted_chunks.csv\")\n",
    "\n",
    "def get_top_k_similar_chunks(question, k=5):\n",
    "    question_embedding = model.encode([question])\n",
    "    similarity = cosine_similarity(question_embedding, embeddings)[0]\n",
    "    top_indices = similarity.argsort()[::-1][:k]\n",
    "\n",
    "    top_chunks = []\n",
    "    for index in top_indices:\n",
    "        chunk = df_chunks.iloc[index].to_dict()\n",
    "        chunk[\"similarity\"] = similarity[index]\n",
    "        top_chunks.append(chunk)\n",
    "    return top_chunks\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user_prompt = \"What was the net profit in FY 2022?\"\n",
    "    result = get_top_k_similar_chunks(user_prompt, k=5)\n",
    "    df_ex = pd.DataFrame(result)\n",
    "    print(df_ex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed8231f-5cc9-460e-88c0-56c2f68d305a",
   "metadata": {},
   "source": [
    "## What's New Compared to Traditional Search ##\n",
    "\n",
    "Feature                       | Ctrl+F in PDF | Current System                             |\n",
    "----------------------------- | ------------- | --------------------------------------- |\n",
    "Requires exact words          | Yes           | No                                      |\n",
    "Understands synonyms          | No            | Yes (e.g., *\"profit\"* ≈ *\"net income\"*) |\n",
    "Works across multiple files   | No            | Yes                                     |\n",
    "Ranks based on meaning        | No            | Yes                                     |\n",
    "Ready for summarization or QA | No            | Yes                                     |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d066d4d-3c0b-47f3-8883-04be68f1fdaa",
   "metadata": {},
   "source": [
    "# Step 4: Prompt Construction #\n",
    "\n",
    "<ul> \n",
    "    <li> \n",
    "        This function takes the top-k retrieved chunks and the user query, then builds a structured prompt with chunk metadata and clear instructions for the LLM to follow. \n",
    "    </li> \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c51e4c10-c765-4afd-94ee-c5c38a370dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def system_prompt(context_chunks, question):\n",
    "    context = \"\"\n",
    "    for index, chunk in enumerate(context_chunks):\n",
    "        context += f\"### Chunk {index+1} (Page {chunk['page']} of {chunk['filename']}):\\n{chunk['text']}\\n\\n\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a financial question-answering assistant working with company annual reports.\n",
    "\n",
    "Only use the information provided in the context below to answer the question. Do not guess. \n",
    "If the answer cannot be found, respond with: \"Not found in the documents.\"\n",
    "\n",
    "-----------------------\n",
    "Context:\n",
    "{context}\n",
    "-----------------------\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    return prompt.strip()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec21a18-cd44-4155-86e6-bb12f745058e",
   "metadata": {},
   "source": [
    "# Step 5: Answer Generation via Local LLM #\n",
    "\n",
    "<ul> \n",
    "    <li> \n",
    "        This function sends the constructed prompt to a local language model [Mistral via Ollama] and returns a grounded answer based strictly on the given context. \n",
    "    </li> \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98a70d69-a460-459d-a45e-c07b3c3b1f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "def local_generate(prompt):\n",
    "    response = requests.post(\n",
    "        'http://localhost:11434/api/generate',\n",
    "        json={\n",
    "            \"model\": \"mistral\",\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False\n",
    "        }\n",
    "    )\n",
    "    return response.json()[\"response\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2ca66e-c7e4-4152-b7ec-6d31f6ff4be1",
   "metadata": {},
   "source": [
    "# Step 6: Response Generation and Display #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd6c0288-43ad-4faa-8d4b-ed042a9c7346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  From the provided context, it is stated that HDFC Bank has been a key player in extending the reach of Government education programmes and enhancing its education loan offering. The bank has removed the cap on committed state education loan values and widened the range of courses for which students can avail loans. There is no explicit mention of the demand for these loans increasing every year, but the context suggests that the bank's focus on education loans indicates an increased emphasis or effort towards it. Therefore, it can be inferred that there might be an increasing trend in demand for education loans, but direct numerical data is not provided in the context to support this conclusion.\n"
     ]
    }
   ],
   "source": [
    "question = \"Give me insights about the Education Loan, is the demand for such loans increasing every year? Whats the general trend happening?\"\n",
    "top_k_chunks = get_top_k_similar_chunks(question, k=5)\n",
    "\n",
    "\n",
    "prompt = system_prompt(top_k_chunks, question)\n",
    "response = local_generate(prompt)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Answer:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccfb9826-8aae-40d5-86a8-14cc84a547f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  The net profit in FY 2022 was ` 36,961.3 crore. (Refer to Chunk 3 from HDFC_Integrated_Annual_Report_2022.pdf)\n"
     ]
    }
   ],
   "source": [
    "question = \"What was the net profit in FY 2022?\"\n",
    "top_k_chunks = get_top_k_similar_chunks(question, k=5)\n",
    "\n",
    "\n",
    "prompt = system_prompt(top_k_chunks, question)\n",
    "response = local_generate(prompt)\n",
    "\n",
    "\n",
    "print(\"Answer:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc76c695-fbfa-49e3-a2fb-bf8441165765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, Im Your Finacial QA Assisatnt. How can i help you?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">> Net Profit FY 2022?\n",
      ">> Best Performing Investment in the year 2024?\n",
      ">> quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All queries have been taken. Please Wait for a while\n"
     ]
    }
   ],
   "source": [
    "User_Prompts = []\n",
    "\n",
    "print(\"Hi, Im Your Finacial QA Assisatnt. How can i help you?\")\n",
    "while True:\n",
    "    Question = input(\">>\")\n",
    "    if Question.lower() == \"quit\":\n",
    "        print(\"All queries have been taken. Please Wait for a while\")\n",
    "        break\n",
    "    else:\n",
    "        User_Prompts.append(Question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c3a60f0-929e-4c2a-9433-0981788862df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Net Profit FY 2022?', 'Best Performing Investment in the year 2024?']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "User_Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e1a7939-b163-4664-be50-d7474c5bcaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net Profit FY 2022?\n",
      "Answer:  The Net Profit for FY 2022 can be found in the Chunk 2 of the provided context. It is 369,613,552 according to the report. This amount is in '000 and should be multiplied by 1000 to get the actual value. So, the Net Profit for FY 2022 is approximately 369,613,552 * 1000 = 369,613,552 (INR).\n",
      "==================================\n",
      "Best Performing Investment in the year 2024?\n",
      "Answer:  Based on the provided data from the annual report for the year 2024 (HDFC_Integrated_Annual_Report_2024.pdf), we can find the best performing investment category under \"Major categories of plan assets as a percentage of fair value of total plan assets\" section:\n",
      "\n",
      "1. Equity shares with 30.34% of fair value to total plan assets.\n",
      "\n",
      "This indicates that equity shares had the highest proportion of the fair value of total plan assets in 2024, suggesting they were likely the best performing investment category compared to government securities, debenture and bonds, and others during that year. However, it is important to note that this is not a definitive statement about returns or profitability without further analysis considering other factors such as investment objectives, risk tolerance, market conditions, etc.\n",
      "\n",
      "Reference: Chunk 4 (Page 467 of HDFC_Integrated_Annual_Report_2024.pdf)\n",
      "==================================\n"
     ]
    }
   ],
   "source": [
    "for question in User_Prompts:\n",
    "    top_k_chunks = get_top_k_similar_chunks(question, k=5)\n",
    "    prompt = system_prompt(top_k_chunks, question)\n",
    "    response = local_generate(prompt)\n",
    "    print(question)\n",
    "    print(\"Answer:\", response)\n",
    "    print(\"==================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a824c66e-48df-4397-b104-102cf396eb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas 2.2.2\n",
      "Numpy 1.26.4\n",
      "Pdfplumber 0.11.4\n",
      "requests 2.32.3\n",
      "sentence_transformers 5.0.0\n",
      "fastapi 0.115.13\n",
      "tqdm 4.66.5\n",
      "sklearn 1.5.1\n",
      "re 2.2.1\n",
      "requests 2.32.3\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "import pdfplumber\n",
    "import requests\n",
    "import sentence_transformers\n",
    "import fastapi\n",
    "import tqdm\n",
    "import sklearn\n",
    "import re\n",
    "import requests\n",
    "\n",
    "\n",
    "print(\"Pandas\", pandas.__version__)\n",
    "print(\"Numpy\", numpy.__version__)\n",
    "print(\"Pdfplumber\", pdfplumber.__version__)\n",
    "print(\"requests\", requests.__version__)\n",
    "print(\"sentence_transformers\", sentence_transformers.__version__)\n",
    "print(\"fastapi\", fastapi.__version__)\n",
    "print(\"tqdm\", tqdm.__version__)\n",
    "print(\"sklearn\", sklearn.__version__)\n",
    "print(\"re\", re.__version__)\n",
    "print(\"requests\", requests.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899858a6-b97c-430c-90cf-2a804fd16f92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1df5f2-8174-46c0-974d-cf6111133626",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b47ca91-0597-4edc-8192-e160dedd05c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
